@MISC{protonet,
  title={Prototypical Networks for Few-shot Learning}, 
  author={Jake Snell and Kevin Swersky and Richard S. Zemel},
  year={2017},
  eprint={1703.05175},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1703.05175}, 
}

% ViTとCNNが重視する特徴
@MISC{feature,
  title={Are Convolutional Neural Networks or Transformers more like human vision?},
  author={Shikhar Tuli and Ishita Dasgupta and Erin Grant and Thomas L. Griffiths},
  year={2021},
  eprint={2105.07197},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

% OSR
@MISC{sun2023survey,
  title={A Survey on Open-Set Image Recognition},
  author={Jiayin Sun and Qiulei Dong},
  year={2023},
  eprint={2312.15571},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

% timm
@MISC{timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}},
  note = {GitHub repository (2025年1月22日閲覧)}
}

% CLIP
@MISC{clip,
  title={Learning Transferable Visual Models From Natural Language Supervision}, 
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  year={2021},
  eprint={2103.00020},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2103.00020}, 
}

% WCS
@MISC{wcs,
  author={Wildlife Conservation Society},
  title = {{WCS Camera Traps}},
  howpublished = {\url{https://lila.science/datasets/wcscameratraps}},
  note = {Labeled Information Library of Alexandria: Biology and Conservation (LILA BC) (2025年1月8日閲覧)}
}

% 生態系サービス
% 自分で作成
@BOOK{millennium-ecosystem2005,
  author       = {Duraiappah, Anantha Kumar and Naeem, Shahid and Agardy, Tundi and Ash, Neville J. and Cooper, H. David and Diaz, Sandra and Faith, Daniel P. and Mace, Georgina and McNeely, Jeffrey A. and Mooney, Harold A. and Oteng-Yeboah, Alfred A. and Pereira, Henrique Miguel and Polasky, Stephen and Prip, Christian and Reid, Walter V. and Samper, Cristian and Schei, Peter Johan and Scholes, Robert and Schutyser, Frederik and van Jaarsveld, Albert},
  title        = {Ecosystems and Human Well-Being: Biodiversity Synthesis},
  publisher    = {World Resources Institute (WRI)},
  address      = {Washington, DC},
  series       = {Millennium Ecosystem Assessment Series},
  year         = {2005},
  pages        = {86},
  isbn         = {1-56973-588-3},
  institution  = {World Resources Institute (WRI)},
  keywords     = {Ecosystem conservation, Ecosystem management, Biological diversity, Human ecology, Social aspects}
}

% 生物多様性国家戦略
@TECHREPORT{biodiversity2023,
  title        = {生物多様性国家戦略2023-2030〜ネイチャーポジディブ実現に向けたロードマップ〜},
  author       = {環境省},
  year         = {2023},
  month        = {3},
  address      = {東京},
  institution  = {環境省 自然環境局},
  howpublished = {\url{https://www.biodic.go.jp/biodiversity/about/initiatives6/files/1_2023-2030text.pdf}},
  note         = {生物多様性センター(2024年12月7日閲覧)},
  language     = {japanese}
}

@ARTICLE{newbold2015,
  author         = {Newbold, Tim and Hudson, Lawrence and Hill, Samantha and Contu, Sara and Lysenko, Igor and Senior, Rebecca and Börger, Luca and Bennett, Dominic and Choimes, Argyrios and Collen, Ben and Day, Julie and De Palma, Adriana and Diaz, Sandra and Echeverria-Londono, Susy and Edgar, Melanie and Feldman, Anat and Garon, Morgan and Harrison, Michelle and Alhusseini, Tamera and Purvis, Andy},
  year           = {2015},
  month          = {04},
  pages          = {45-50},
  title          = {Global effects of land use on local terrestrial biodiversity},
  volume         = {520},
  journal        = {Nature},
  doi            = {10.1038/nature14324}
}

@ARTICLE{isbell2017,
  author = {Isbell, Forest and Gonzalez, Andrew and Loreau, Michel and Cowles, Jane and Diaz, Sandra and Hector, Andy and Wardle, David and O’Connor, Mary and Duffy, J. and Turnbull, Lindsay and Thompson, Patrick and Larigauderie, Anne},
  year = {2017},
  month = {06},
  pages = {65-72},
  title = {Linking the influence and dependence of people on biodiversity across scales},
  volume = {546},
  journal = {Nature},
  doi = {10.1038/nature22899}
}

% YOLO-V5
@article{yolov5,
  author = {Fang, Yiming and Guo, Xianxin and Chen, Kun and Zhou, Zhu and Ye, Qing},
  year = {2021},
  month = {06},
  pages = {5390-5406},
  title = {Accurate and automated detection of surface knots on sawn timbers using YOLO-V5 model},
  volume = {16},
  journal = {BioResources},
  doi = {10.15376/biores.16.3.5390-5406}
}

% 生態系サービス
@ARTICLE{cardinale2011,
author = {Cardinale, Bradley and Matulich, Kristin and Hooper, David and Byrnes, Jarrett and Duffy, J. and Gamfeldt, Lars and Balvanera, Patricia and O'Connor, Mary and Gonzalez, Andrew},
year = {2011},
month = {03},
pages = {572-92},
title = {The functional role of producer diversity in ecosystems},
volume = {98},
journal = {American journal of botany},
doi = {10.3732/ajb.1000364}
}

% カメラトラップ
@ARTICLE{newey2015,
  author = {Newey, Scott and Davidson G, Paul and Nazir, Sajid and Fairhurst, Gorry and Verdicchio, Fabio and Irvine, Robert and van der Wal, Rene},
  year = {2015},
  month = {11},
  pages = {624-635},
  title = {Limitations of recreational camera traps for wildlife management and conservation research: A practitioner’s perspective},
  volume = {44},
  journal = {Ambio},
  doi = {10.1007/s13280-015-0713-1}
}

% カメラトラップ
@ARTICLE{jia2022,
  author = {Jia, Liang and Tian, Ye and Zhang, Junguo},
  year = {2022},
  month = {02},
  pages = {437},
  title = {Domain-Aware Neural Architecture Search for Classifying Animals in Camera Trap Images},
  volume = {12},
  journal = {Animals},
  doi = {10.3390/ani12040437}
}

% カメラトラップ
@ARTICLE{carl2020,
  author = {Carl, Christin and Schönfeld, Fiona and Profft, Ingolf and Klamm, Alisa and Landgraf, Dirk},
  title = {Automated detection of {European} wild mammal species in camera trap images with an existing and pre-trained computer vision model},
  journal = {European Journal of Wildlife Research},
  volume = {66},
  year = {2020},
  number = {62},
  pages = {},
  doi = {10.1007/s10344-020-01404-y}
}

% 動物分類
@ARTICLE{tan2022,
  author = {Tan, Mengyu and Chao, Wentao and Cheng, Jo-Ku and Zhou, Mo and Ma, Yiwen and Jiang, Xinyi and Ge, Jianping and Yu, Lian and Feng, Limin},
  title = {Animal Detection and Classification from Camera Trap Images Using Different Mainstream Object Detection Architectures},
  journal = {Animals},
  volume = {12},
  year = {2022},
  number = {15},
  article-number = {1976},
  PubMedID = {35953964},
  issn = {2076-2615},
  abstract = {Camera traps are widely used in wildlife surveys and biodiversity monitoring. Depending on its triggering mechanism, a large number of images or videos are sometimes accumulated. Some literature has proposed the application of deep learning techniques to automatically identify wildlife in camera trap imagery, which can significantly reduce manual work and speed up analysis processes. However, there are few studies validating and comparing the applicability of different models for object detection in real field monitoring scenarios. In this study, we firstly constructed a wildlife image dataset of the Northeast Tiger and Leopard National Park (NTLNP dataset). Furthermore, we evaluated the recognition performance of three currently mainstream object detection architectures and compared the performance of training models on day and night data separately versus together. In this experiment, we selected YOLOv5 series models (anchor-based one-stage), Cascade R-CNN under feature extractor HRNet32 (anchor-based two-stage), and FCOS under feature extractors ResNet50 and ResNet101 (anchor-free one-stage). The experimental results showed that performance of the object detection models of the day-night joint training is satisfying. Specifically, the average result of our models was 0.98 mAP (mean average precision) in the animal image detection and 88% accuracy in the animal video classification. One-stage YOLOv5m achieved the best recognition accuracy. With the help of AI technology, ecologists can extract information from masses of imagery potentially quickly and efficiently, saving much time.},
  doi = {10.3390/ani12151976},
  url = {https://www.mdpi.com/2076-2615/12/15/1976}
}

% 動物分類
@ARTICLE{schneider2020,
  author = {Schneider, Stefan and Greenberg, Saul and Taylor, Graham and Kremer, Stefan},
  title = {Three critical factors affecting automated image species recognition performance for camera traps},
  journal = {Ecology and Evolution},
  volume = {10},
  number = {7},
  year = {2020},
  pages = {3503--3517},
  doi = {10.1002/ece3.6147}
}

% 動物分類
@ARTICLE{manna2023,
  title = {Bird Image Classification using Convolutional Neural Network Transfer Learning Architectures},
  journal = {International Journal of Advanced Computer Science and Applications},
  doi = {10.14569/IJACSA.2023.0140397},
  url = {http://dx.doi.org/10.14569/IJACSA.2023.0140397},
  year = {2023},
  publisher = {The Science and Information Organization},
  volume = {14},
  number = {3},
  author = {Asmita Manna and Nilam Upasani and Shubham Jadhav and Ruturaj Mane and Rutuja Chaudhari and Vishal Chatre}
}

@ARTICLE{mohanty2022, 
  place={Houston, USA}, 
  title={Fish Species Image Classification Using Convolutional Neural Networks}, 
  volume={11}, 
  url={https://www.jsr.org/hs/index.php/path/article/view/3058}, 
  DOI={10.47611/jsrhs.v11i3.3058}, 
  abstractNote={&lt;p&gt;This paper demonstrates the classification of various fish species using different machine learning methods. By incorporating machine learning algorithms, modeling, and training, the project classifies fish species using neural networks with the help of multiple features like length, width, and more. Ultimately, this project attempts to analyze the differences between determining fish species with PyTorch and TensorFlow. Convolutional Neural Networks (CNN) is a powerful algorithm used in image classification problems. Python has various libraries which can be used to build a model for the same purpose; the ultimate goal of this study is to see whether using different libraries will affect the accuracy. I would like to see whether new and more advanced methods can be used to classify large schools of fish rather than only in labs. I developed separate Python codes using PyTorch and TensorFlow individually. Using each code, I obtained results and, in the end, performed a comparative study between both to come to my conclusion. My main findings were that PyTorch gave a more accurate prediction than TensorFlow. I believe this was the case because the PyTorch code incorporated neural networks with more layers, so it increased the training and validation accuracy. From here, it is evident that while neither method necessarily possesses setbacks, PyTorch has a significant edge in accuracy (99.75% to 87.22%). Therefore, when scientists apply classification with CNN, PyTorch may be more optimal for producing better results.&lt;/p&gt;}, 
  number={3}, 
  journal={Journal of Student Research}, 
  author={Mohanty, Anishka and Goldsztein, Guillermo and Pellegrin, Raphaël}, 
  year={2022}, 
  month={Aug.}
}

% 赤外線動物画像に対する既存研究
@ARTICLE{tabak2019,
  author = {Tabak, Michael A. and Norouzzadeh, Mohammad S. and Wolfson, David W. and Sweeney, Steven J. and Vercauteren, Kurt C. and Snow, Nathan P. and Halseth, Joseph M. and Di Salvo, Paul A. and Lewis, Jesse S. and White, Michael D. and Teton, Ben and Beasley, James C. and Schlichting, Peter E. and Boughton, Raoul K. and Wight, Bethany and Newkirk, Eric S. and Ivan, Jacob S. and Odell, Eric A. and Brook, Ryan K. and Lukacs, Paul M. and Moeller, Anna K. and Mandeville, Elizabeth G. and Clune, Jeff and Miller, Ryan S.},
  title = {Machine learning to classify animal species in camera trap images: Applications in ecology},
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {4},
  pages = {585-590},
  keywords = {artificial intelligence, camera trap, convolutional neural network, deep neural networks, image classification, machine learning, r package, remote sensing},
  doi = {https://doi.org/10.1111/2041-210X.13120},
  url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13120},
  eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13120},
  abstract = {Abstract Motion-activated cameras (“camera traps”) are increasingly used in ecological and management studies for remotely observing wildlife and are amongst the most powerful tools for wildlife research. However, studies involving camera traps result in millions of images that need to be analysed, typically by visually observing each image, in order to extract data that can be used in ecological analyses. We trained machine learning models using convolutional neural networks with the ResNet-18 architecture and 3,367,383 images to automatically classify wildlife species from camera trap images obtained from five states across the United States. We tested our model on an independent subset of images not seen during training from the United States and on an out-of-sample (or “out-of-distribution” in the machine learning literature) dataset of ungulate images from Canada. We also tested the ability of our model to distinguish empty images from those with animals in another out-of-sample dataset from Tanzania, containing a faunal community that was novel to the model. The trained model classified approximately 2,000 images per minute on a laptop computer with 16 gigabytes of RAM. The trained model achieved 98\% accuracy at identifying species in the United States, the highest accuracy of such a model to date. Out-of-sample validation from Canada achieved 82\% accuracy and correctly identified 94\% of images containing an animal in the dataset from Tanzania. We provide an r package (Machine Learning for Wildlife Image Classification) that allows the users to (a) use the trained model presented here and (b) train their own model using classified images of wildlife from their studies. The use of machine learning to rapidly and accurately classify wildlife in camera trap images can facilitate non-invasive sampling designs in ecological studies by reducing the burden of manually analysing images. Our r package makes these methods accessible to ecologists.},
  year = {2019}
}

@ARTICLE{osr,
  author = {Scheirer, Walter and Rocha, Anderson and Sapkota, Archana and Boult, Terrance},
  year = {2013},
  month = {07},
  pages = {1757-72},
  title = {Toward Open Set Recognition},
  volume = {35},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  doi = {10.1109/TPAMI.2012.256}
}

% 生態系モニタリングの重要性
@ARTICLE{zwerts2021,
  author = {Zwerts, Joeri and Stephenson, PJ and Maisels, Fiona and Rowcliffe, Marcus and Astaras, Christos and Jansen, Patrick and van der Waarde, Jaap and Sterck, Liesbeth and Verweij, P.A. and Bruce, Tom and Brittain, Stephanie and Kuijk, Marijke},
  year = {2021},
  month = {11},
  pages = {},
  title = {Methods for wildlife monitoring in tropical forests: Comparing human observations, camera traps, and passive acoustic sensors},
  volume = {3},
  journal = {Conservation Science and Practice},
  doi = {10.1111/csp2.568}
}

% camera trap
@ARTICLE{trolliet2014,
  author = {Trolliet, Franck and Huynen, Marie-Claude and Vermeulen, Cédric and Hambuckers, Alain},
  year = {2014},
  month = {01},
  pages = {446-454},
  title = {Use of camera traps for wildlife studies. A review},
  volume = {18},
  journal = {Biology Agriculture Science Environnement}
}

% camera trap
@ARTICLE{本郷2018,
  title={霊長類学におけるカメラトラップ研究},
  author={本郷 峻},
  journal={霊長類研究},
  volume={34},
  number={1},
  pages={53-64},
  year={2018},
  doi={10.2354/psj.34.014}
}

% camera trap
@ARTICLE{安藤2019,
  title={深層学習（{Deep Learning}）によるカメラトラップ画像の判別},
  author={安藤 正規 and 中塚 俊介 and 相澤 宏旭 and 中森 さつき and 池田 敬 and 森部 絢嗣 and 寺田 和憲 and 加藤 邦人},
  journal={哺乳類科学},
  volume={59},
  number={1},
  pages={49-60},
  year={2019},
  doi={10.11238/mammalianscience.59.49}
}

@ARTICLE{kays2020,
  author = {Kays, Roland and Arbogast, Brian S. and Baker-Whatton, Megan and Beirne, Chris and Boone, Hailey M. and Bowler, Mark and Burneo, Santiago F. and Cove, Michael V. and Ding, Ping and Espinosa, Santiago and Gonçalves, André Luis Sousa and Hansen, Christopher P. and Jansen, Patrick A. and Kolowski, Joseph M. and Knowles, Travis W. and Lima, Marcela Guimarães Moreira and Millspaugh, Joshua and McShea, William J. and Pacifici, Krishna and Parsons, Arielle W. and Pease, Brent S. and Rovero, Francesco and Santos, Fernanda and Schuttler, Stephanie G. and Sheil, Douglas and Si, Xingfeng and Snider, Matt and Spironello, Wilson R.},
  title = {An empirical evaluation of camera trap study design: How many, how long and when?},
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {6},
  pages = {700-713},
  keywords = {camera traps, community ecology, detectability, mammals, relative abundance, species richness, study design, wildlife surveys},
  doi = {https://doi.org/10.1111/2041-210X.13370},
  url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13370},
  eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13370},
  abstract = {Abstract Camera traps deployed in grids or stratified random designs are a well-established survey tool for wildlife but there has been little evaluation of study design parameters. We used an empirical subsampling approach involving 2,225 camera deployments run at 41 study areas around the world to evaluate three aspects of camera trap study design (number of sites, duration and season of sampling) and their influence on the estimation of three ecological metrics (species richness, occupancy and detection rate) for mammals. We found that 25–35 camera sites were needed for precise estimates of species richness, depending on scale of the study. The precision of species-level estimates of occupancy (ψ) was highly sensitive to occupancy level, with <20 camera sites needed for precise estimates of common (ψ > 0.75) species, but more than 150 camera sites likely needed for rare (ψ < 0.25) species. Species detection rates were more difficult to estimate precisely at the grid level due to spatial heterogeneity, presumably driven by unaccounted habitat variability factors within the study area. Running a camera at a site for 2 weeks was most efficient for detecting new species, but 3–4 weeks were needed for precise estimates of local detection rate, with no gains in precision observed after 1 month. Metrics for all mammal communities were sensitive to seasonality, with 37\%–50\% of the species at the sites we examined fluctuating significantly in their occupancy or detection rates over the year. This effect was more pronounced in temperate sites, where seasonally sensitive species varied in relative abundance by an average factor of 4–5, and some species were completely absent in one season due to hibernation or migration. We recommend the following guidelines to efficiently obtain precise estimates of species richness, occupancy and detection rates with camera trap arrays: run each camera for 3–5 weeks across 40–60 sites per array. We recommend comparisons of detection rates be model based and include local covariates to help account for small-scale variation. Furthermore, comparisons across study areas or times must account for seasonality, which could have strong impacts on mammal communities in both tropical and temperate sites.},
  year = {2020}
}

@ARTICLE{si2014,
  title = {How long is enough to detect terrestrial animals? Estimating the minimum trapping effort on camera traps},
  author = {Si, Xingfeng and Kays, Roland and Ding, Ping},
  year = 2014,
  month = may,
  keywords = {Animal inventory, Species richness, Gutianshan, Wildlife monitoring, Species accumulation curves, Sampling effort, Camera day},
  abstract = {
        Camera traps is an important wildlife inventory tool for estimating species diversity at a site. Knowing what minimum trapping effort is needed to detect target species is also important to designing efficient studies, considering both the number of camera locations, and survey length. Here, we take advantage of a two-year camera trapping dataset from a small (24-ha) study plot in Gutianshan National Nature Reserve, eastern China to estimate the minimum trapping effort actually needed to sample the wildlife community. We also evaluated the relative value of adding new camera sites or running cameras for a longer period at one site. The full dataset includes 1727 independent photographs captured during 13,824 camera days, documenting 10 resident terrestrial species of birds and mammals. Our rarefaction analysis shows that a minimum of 931 camera days would be needed to detect the resident species sufficiently in the plot, and \textit{c}. 8700 camera days to detect all 10 resident species. In terms of detecting a diversity of species, the optimal sampling period for one camera site was \textit{c}. 40, or long enough to record about 20 independent photographs. Our analysis of evaluating the increasing number of additional camera sites shows that rotating cameras to new sites would be more efficient for measuring species richness than leaving cameras at fewer sites for a longer period.
      },
  volume = 2,
  pages = {e374},
  journal = {PeerJ},
  issn = {2167-8359},
  url = {https://doi.org/10.7717/peerj.374},
  doi = {10.7717/peerj.374}
}

% OSR
@ARTICLE{geng2021survey,
  author={Geng, Chuanxing and Huang, Sheng-Jun and Chen, Songcan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Recent Advances in Open Set Recognition: A Survey}, 
  year={2021},
  volume={43},
  number={10},
  pages={3614-3631},
  keywords={Training;Testing;Task analysis;Semantics;Face recognition;Data visualization;Open set recognition/classification;open world recognition;zero-short learning;one-shot learning},
  doi={10.1109/TPAMI.2020.2981604}
}

% CNN
@ARTICLE{mohanty2016,
  AUTHOR={Mohanty, Sharada P.  and Hughes, David P.  and Salathé, Marcel },
  TITLE={Using Deep Learning for Image-Based Plant Disease Detection},
  JOURNAL={Frontiers in Plant Science},
  VOLUME={7},
  YEAR={2016},
  URL={https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2016.01419},
  DOI={10.3389/fpls.2016.01419},
  ISSN={1664-462X},
  ABSTRACT={<p>Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.</p>}
}

% CNN
@ARTICLE{sue2020,
  title = {New perspectives on plant disease characterization based on deep learning},
  journal = {Computers and Electronics in Agriculture},
  volume = {170},
  pages = {105220},
  year = {2020},
  issn = {0168-1699},
  doi = {https://doi.org/10.1016/j.compag.2020.105220},
  url = {https://www.sciencedirect.com/science/article/pii/S0168169919300560},
  author = {Sue Han Lee and Hervé Goëau and Pierre Bonnet and Alexis Joly},
  keywords = {Plant diseases, Automated visual crops analysis, Deep learning, Transfer learning},
  abstract = {The control of plant diseases is a major challenge to ensure global food security and sustainable agriculture. Several recent studies have proposed to improve existing procedures for early detection of plant diseases through modern automatic image recognition systems based on deep learning. In this article, we study these methods in detail, especially those based on convolutional neural networks. We first examine whether it is more relevant to fine-tune a pre-trained model on a plant identification task rather than a general object recognition task. In particular, we show, through visualization techniques, that the characteristics learned differ according to the approach adopted and that they do not necessarily focus on the part affected by the disease. Therefore, we introduce a more intuitive method that considers diseases independently of crops, and we show that it is more effective than the classic crop-disease pair approach, especially when dealing with disease involving crops that are not illustrated in the training database. This finding therefore encourages future research to rethink the current de facto paradigm of crop disease categorization.}
}

% AlexNet
@INPROCEEDINGS{alexnet,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

% FCOS
@INPROCEEDINGS{fcos,
  author = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
  title = {FCOS: Fully Convolutional One-Stage Object Detection},
  booktitle = {IEEE/CVF International Conference on Computer Vision (ICCV)},
  month = {October},
  year = {2019}
}

% Cascade R-CNN
@INPROCEEDINGS{cascade,
  author={Cai, Zhaowei and Vasconcelos, Nuno},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Cascade R-CNN: Delving Into High Quality Object Detection}, 
  year={2018},
  volume={},
  number={},
  pages={6154-6162},
  keywords={Detectors;Object detection;Proposals;Training;Computer architecture;Task analysis;Noise measurement},
  doi={10.1109/CVPR.2018.00644}
}

% camera trap
@INPROCEEDINGS{thangaraj2023,
  author={Thangaraj, Rajasekaran and Rajendar, Sivaramakrishnan and M, Sanjith and K, Rithick Saran and Sasikumar, Sudev and L, Chandhru},
  booktitle={Third International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)}, 
  title={Automated Recognition of Wild Animal Species in Camera Trap Images Using Deep Learning Models}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Deep learning;Image recognition;Biological system modeling;Wildlife;Ecosystems;Transfer learning;Cameras;Animal species recognition;Convolution neural network;Inception V3;camera trap;transfer learning;fine-tuning;Data Augmentation},
  doi={10.1109/ICAECT57570.2023.10117922}
}

camera trap
@INPROCEEDINGS{abood2023,
  author={Abood, Baydaa Sh. Z. and M, Manjula B. and Almoussawi, Zainab abed and Shilpa, N and Shakir, Aboothar mahmood},
  booktitle={3rd International Conference on Mobile Networks and Wireless Communications (ICMNWC)}, 
  title={Revolutionizing Wildlife Monitoring: A Novel Approach to Camera Trap Image Analysis with YOLOv5}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  keywords={YOLO;Training;Biological system modeling;Wildlife;Real-time systems;Environmental management;Monitoring;Deep Learning;Deep Neural Network;Regions-Convolutional Neural Network;Wile Life Detection and YOLO v5},
  doi={10.1109/ICMNWC60182.2023.10435785}
}

% 生態系モニタリングの重要性
@INPROCEEDINGS{bandaru2024,
  author={Bandaru, Jabili and Basa, Nikitha and Raghavendra, P and Sirisha, A.},
  booktitle={International Conference on Knowledge Engineering and Communication Systems (ICKECS)}, 
  title={Review on Various Techniques for Wildlife Monitoring and Alerting Systems}, 
  year={2024},
  volume={1},
  number={},
  pages={1-5},
  keywords={YOLO;Time-frequency analysis;Reviews;Wildlife;Habitats;Video sequences;Internet of Things;YOLO;IOT;Deep learning;Deep Neural Networks},
  doi={10.1109/ICKECS61492.2024.10616522}
}

% 動物分類
@INPROCEEDINGS{agarwal2023,
  author={Agarwal, Nikita and Kalita, Tina and Dubey, Ashwani Kumar},
  booktitle={International Conference on Computational Intelligence and Sustainable Engineering Solutions (CISES)}, 
  title={Classification of Insect Pest Species using CNN based Models}, 
  year={2023},
  volume={},
  number={},
  pages={862-866},
  keywords={Training;Deep learning;Planets;Insects;Sociology;Real-time systems;Mobile applications;Deep Learning;insect identification;insect pest;image classification;IP102},
  doi={10.1109/CISES58720.2023.10183545}
}

% 動物分類
@INPROCEEDINGS{neeli2023,
  author={Neeli, Subash and Guruguri, Chandra Sekhar Reddy and Kammara, Adithya Ram Achari and Annepu, Visalakshi and Bagadi, Kalapraveen and Chirra, Venkata Rami Reddy},
  booktitle={International Conference on Next Generation Electronics (NEleX)}, 
  title={Bird Species Detection Using CNN and EfficientNet-B0}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  keywords={Biological system modeling;Computational modeling;Transfer learning;Computer architecture;Birds;Convolutional neural networks;Monitoring;CNN;EfficientNet-B0;Hierarchical traits Categorization;Bird Species},
  doi={10.1109/NEleX59773.2023.10420966}
}

% カメラトラップ
@INPROCEEDINGS{zhu2017,
  author={Zhu, Chunbiao and Li, Thomas H. and Li, Ge},
  booktitle={IEEE International Conference on Computer Vision Workshops (ICCVW)}, 
  title={Towards Automatic Wild Animal Detection in Low Quality Camera-Trap Images Using Two-Channeled Perceiving Residual Pyramid Networks}, 
  year={2017},
  volume={},
  number={},
  pages={2860-2864},
  keywords={Wildlife;Feature extraction;Cameras;Image segmentation;Computer vision;Training},
  doi={10.1109/ICCVW.2017.337}
}

% カメラトラップ
@INPROCEEDINGS{schneider2018,
  author={Schneider, Stefan and Taylor, Graham W. and Kremer, Stefan},
  booktitle={15th Conference on Computer and Robot Vision (CRV)}, 
  title={Deep Learning Object Detection Methods for Ecological Camera Trap Data}, 
  year={2018},
  volume={},
  number={},
  pages={321-328},
  keywords={Cameras;Object detection;Animals;Computer vision;Task analysis;Sociology;Camera Trap;Object Detector;Transfer Learning;Ecology;Faster R CNN;YOLO;Snapshot Serengeti;Deep Learning;Convolutional Neural Network},
  doi={10.1109/CRV.2018.00052}
}

% 赤外線画像
@INPROCEEDINGS{kishimoto2023,
  author = {Kishi, Koki and Kishimoto, Masako and Situju, Sulfayanti and Takimoto, Hironori and Kanagawa, Akihiro},
  booktitle = {10th IIAE International Conference on Intelligent Systems and Image Processing (ICISIP)},
  title = {Few-Shot Learning for {CNN}-based Animal Classification in Camera Traps using an Infrared Camera},
  year = {2023},
  pages = {11-17},
  doi = {10.12792/icisip2023.005}
}

% OSR
@INPROCEEDINGS{mahdavi2021survey,
  author={Mahdavi, Atefeh and Carvalho, Marco},
  booktitle={IEEE Fourth International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)}, 
  title={A Survey on Open Set Recognition}, 
  year={2021},
  volume={},
  number={},
  pages={37-44},
  keywords={Training;Knowledge engineering;Conferences;Training data;Machine learning;Learning (artificial intelligence);Multitasking;machine learning;classification;open set recognition;multi-task learning;risk of the unknown},
  doi={10.1109/AIKE52691.2021.00013}
}

% OSR
@INPROCEEDINGS{sun2020,
  author={Sun, Xin and Yang, Zhenning and Zhang, Chi and Ling, Keck-Voon and Peng, Guohao},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Conditional Gaussian Distribution Learning for Open Set Recognition}, 
  year={2020},
  volume={},
  number={},
  pages={13477-13486},
  keywords={Feature extraction;Training;Task analysis;Testing;Probabilistic logic;Decoding;Anomaly detection},
  doi={10.1109/CVPR42600.2020.01349}
}

% OSR
@INPROCEEDINGS{sagar2022,
  title={Open-Set Recognition: A Good Closed-Set Classifier is All You Need},
  author={Sagar Vaze and Kai Han and Andrea Vedaldi and Andrew Zisserman},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2022},
url={https://openreview.net/forum?id=5hLP5JY9S2d}
}

% FSOSR
@INPROCEEDINGS{peeler,
  author={Liu, Bo and Kang, Hao and Li, Haoxiang and Hua, Gang and Vasconcelos, Nuno},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Few-Shot Open-Set Recognition Using Meta-Learning}, 
  year={2020},
  volume={},
  number={},
  pages={8795-8804},
  keywords={Training;Measurement;Task analysis;Robustness;Entropy;Image recognition;Face recognition},
  doi={10.1109/CVPR42600.2020.00882}}

% FSOSR
@INPROCEEDINGS{snatcher,
  author={Jeong, Minki and Choi, Seokeon and Kim, Changick},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title={Few-shot Open-set Recognition by Transformation Consistency}, 
  year={2021},
  volume={},
  number={},
  pages={12561-12570},
  keywords={Learning systems;Computer vision;Adaptation models;Prototypes;Estimation;Detectors;Pattern recognition},
  doi={10.1109/CVPR46437.2021.01238}}

% FSOSR
@INPROCEEDINGS{tane,
  author    = {Huang, Shiyuan and Ma, Jiawei and Han, Guangxing and Chang, Shih-Fu},
  title     = {Task-Adaptive Negative Envision for Few-Shot Open-Set Recognition},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2022},
  pages     = {7171-7180}
}

% FSOSR
@INPROCEEDINGS{wang2023,
  author={Wang, Haoyu and Pang, Guansong and Wang, Peng and Zhang, Lei and Wei, Wei and Zhang, Yanning},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Glocal Energy-based Learning for Few-Shot Open-Set Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={7507-7516},
  keywords={Measurement;Computer vision;Feature extraction;Pattern recognition;Task analysis;Standards;Recognition: Categorization;detection;retrieval},
  doi={10.1109/CVPR52729.2023.00725}
}

% FSOSR
@INPROCEEDINGS{che2023,
  title     = {Boosting Few-Shot Open-Set Recognition with Multi-Relation Margin Loss},
  author    = {Che, Yongjuan and An, Yuexuan and Xue, Hui},
  booktitle = {Thirty-Second International Joint Conference on Artificial Intelligence, {IJCAI-23}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Edith Elkind},
  pages     = {3505--3513},
  year      = {2023},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2023/390},
  url       = {https://doi.org/10.24963/ijcai.2023/390},
}

% ViT
@INPROCEEDINGS{vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021},
  url={https://openreview.net/forum?id=YicbFdNTTy}
}

% FDSL
@ARTICLE{fdsl,
  author = {Kataoka, Hirokatsu and Okayasu, Kazushige and Matsumoto, Asato and Yamagata, Eisuke and Yamada, Ryosuke and Inoue, Nakamasa and Nakamura, Akio and Satoh, Yutaka},
  title = {Pre-training without Natural Images},
  journal = {International Journal of Computer Vision (IJCV)},
  year = {2022}
}

% k-means loss
@INPROCEEDINGS{k-means,
  author={Tsai, Chin-Chia and Wu, Tsung-Hsuan and Lai, Shang-Hong},
  booktitle={IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Multi-Scale Patch-Based Representation Learning for Image Anomaly Detection and Segmentation}, 
  year={2022},
  volume={},
  number={},
  pages={3992--4000},
  keywords={Representation learning;Training;Image segmentation;Computer vision;Image representation;Benchmark testing;Feature extraction;Industrial Inspection Transfer;Few-shot;Semi- and Un- supervised Learning},
  doi={10.1109/WACV51458.2022.00312}
}

% ResNet
@INPROCEEDINGS{resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  keywords={Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
  doi={10.1109/CVPR.2016.90}
}

% DeiT
@INPROCEEDINGS{deit,
  title = 	 {Training data-efficient image transformers \& distillation through attention},
  author =       {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  booktitle = 	 {38th International Conference on Machine Learning (ICML)},
  pages = 	 {10347--10357},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  month = 	 {18--24 Jul},
  publisher =    {Proceedings of Machine Learning Research (PMLR)},
  pdf = 	 {http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/touvron21a.html},
  abstract = 	 {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.}
}

% CCT
@INPROCEEDINGS{cct,
  author = {Beery, Sara and Van Horn, Grant and Perona, Pietro},
  title = {Recognition in Terra Incognita},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year = {2018}
}