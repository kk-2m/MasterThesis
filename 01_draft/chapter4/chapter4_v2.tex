% History
% 12/06/2024  (岸)	修論下書き用texファイル作成
% 12/12/2024  (岸)	フォントサイズを11pt, 行間を1.5に設定
% コンパイルの仕方
% 		uplatex chapter1_v1.tex
% 		upbibtex chapter1_v1
% 		uplatex chapter1_v1.tex
% 		uplatex chapter1_v1.tex
% 		dvipdfmx chapter1_v1.dvi

% フォントサイズを11ptに設定
\documentclass[a4paper,11pt,nomag]{jsreport}

\usepackage[dvipdfm,truedimen]{geometry}
\geometry{top=22mm,bottom=22mm,left=22mm,right=22mm}
%% jsclasses系で文字サイズ11pt や 12pt をクラスオプションに指定すると，
%% 長さが拡大されるため，nomagオプションを併用している．
%% https://oku.edu.mie-u.ac.jp/~okumura/jsclasses/ のFAQをよく読むこと．

%\usepackage{layout}
%\usepackage[utf8]{inputenc} %不要かも
\usepackage[T1]{fontenc} %utf8フォントエンコーディング指定
\usepackage{lmodern} % 11pt, nomag を使っているので
% CloudLaTeX の場合は下の1行を有効にすること
% \AtBeginDvi{\special{pdf:mapfile ptex-ipaex.map}}
\usepackage{array}
\newcommand{\bhline}[1]{\noalign{\hrule height #1}}  
\newcommand{\bvline}[1]{\vrule width #1}
\renewcommand{\baselinestretch}{1.5} % 教授が赤修正を入れやすいように行間を1.5に設定
\usepackage[singlespacing]{setspace} % 部分的な行間調整パッケージ．表など行間を1.5倍にしたくないところに使う．
\usepackage[subrefformat=parens]{subcaption}
\usepackage[dvipdfmx]{graphicx} % dvipdfmx を前提としている
\usepackage[dvipdfmx]{color}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{here} % 図表の位置決め用
\usepackage{amsmath,amssymb}% 数式用
\usepackage{bbm}
\usepackage{bm} % 数式太字
\usepackage{url}      % URL等記載用．\verbより便利
\usepackage{enumerate}
\usepackage{midpage}
% ハイパーリンク
\usepackage[dvipdfmx,breaklinks=true,colorlinks]{hyperref} % dvipdfmxは日本語のときのみかく
\usepackage{pxjahyper} % (u)pLaTeXのときのみかく

% サブキャプションのフォーマットを調整
\renewcommand\thesubfigure{(\alph{subfigure})}
\captionsetup[subfigure]{labelformat=simple, labelsep=space}

\begin{document}
\setcounter{chapter}{4}

\chapter*{評価実験}

\section{データセット}

IFOR手法の開発において，地域間のドメインシフトを考慮したモデルの性能評価は重要である．
このドメインシフトを実現するため，学習用と評価用のデータセットを異なる地域から選定した．
具体的には，学習用データセットとして南米の野生動物画像を集めたWCS Camera Traps (WCS) \cite{wcs}を使用し，
評価用データセットとして北米の野生動物画像から構成されるCaltech Camera Traps (CCT) \cite{cct}を採用した．

本実験では，提案手法の有効性を多角的に検証するため，各データセットにおいて赤外線画像と，可視光画像それぞれから構成される2つのデータセットを作成した．
このアプローチにより，提案手法が赤外線画像特有の性質に対して有効であるか，
あるいは，赤外線画像や可視光画像を問わず広義の画像分類に対して有効であるかを検証することが可能となる．

データセットの前処理では，各画像を赤外線画像と可視光画像に分類し，
アノテーションとして提供されているバウンディングボックスに基づいてクロッピングを行い，動物が存在している領域を切り出した．
その後，実験の公平性を確保するため，赤外線画像と可視光画像による学習用データセット間でクラス数と画像枚数を同数に統一した．
同様に，評価用データセットについてもクラス数と画像枚数を同一に設定した．
具体的には，学習用データセットは，28クラス，各クラス500枚ずつの合計14,000枚の画像で構成され，
評価用データセットは11クラス，各クラス100枚ずつの合計1,100枚の画像により構成される．
これらの処理により，学習用データセットとして赤外線画像および可視光画像による2種類の WCS データセット，
評価用データセットとして赤外線画像および可視光画像による2種類の CCT データセットの計4つのデータセットを作成した．
ただし，画像の前処理過程において，学習用データセットでは赤外線画像が13,687枚，可視光画像が13,812枚と若干の不均衡が生じたが，
この僅かな差異は実験結果に重大な影響を及ぼさないと考えられる．
これらの前処理によって作成された各データセットの詳細を表 \ref{tbl:wcs}および表 \ref{tbl:cct}に示す．

\begin{table}[tbp]
  \centering
  \caption{WCS Camera Trapsデータセットの画像枚数}
  \label{tbl:wcs}
  \begin{tabular}{c|c|c}
      \hline
      動物種                    & 赤外線画像     & 可視光画像    \\ \hline\hline
      acryllium vulturinum     & 500          & 500          \\
      aepyceros melampus       & 500          & 500          \\
      bos taurus               & 500          & 500          \\
      capra aegagrus           & \textbf{337} & 500          \\
      cephalophus nigrifrons   & 500          & 500          \\
      crax rubra               & 500          & 500          \\
      cuniculus paca           & 500          & \textbf{495} \\
      dasyprocta punctata      & 500          & 500          \\
      equus quagga             & 500          & 500          \\
      giraffa camelopardalis   & 500          & 500          \\
      leopardus pardalis       & 500          & 500          \\
      loxodonta africana       & 500          & 500          \\
      madoqua guentheri        & 500          & 500          \\
      mazama americana         & 500          & 500          \\
      mazama temama            & 500          & 500          \\
      meleagris ocellata       & 500          & 500          \\
      mitu tuberosum           & 500          & 500          \\
      nasua narica             & \textbf{350} & 500          \\
      panthera onca            & 500          & 500          \\
      papio anubis             & 500          & 500          \\
      pecari tajacu            & 500          & 500          \\
      phacochoerus africanus   & 500          & \textbf{317} \\
      psophia leucoptera       & 500          & 500          \\
      puma concolor            & 500          & 500          \\
      syncerus caffer          & 500          & 500          \\
      tapirus terrestris       & 500          & 500          \\
      tayassu pecari           & 500          & 500          \\
      urocyon cinereoargenteus & 500          & 500          \\ \hline\hline
      合計                      & 13,687       & 13,812       \\ \hline
  \end{tabular}
\end{table}

\begin{table}[tbp]
  \centering
  \caption{Caltech Camera Trapsデータセットの画像枚数}
  \label{tbl:cct}
  \begin{tabular}{c|c|c}
      \hline
      動物種       & 赤外線画像 & 可視光画像 \\ \hline\hline
      bird        & 100      & 100       \\
      bobcat      & 100      & 100       \\
      cat         & 100      & 100       \\
      coyote      & 100      & 100       \\
      deer        & 100      & 100       \\
      dog         & 100      & 100       \\
      fox         & 100      & 100       \\
      opossum     & 100      & 100       \\
      rabbit      & 100      & 100       \\
      skunk       & 100      & 100       \\
      squirrel    & 100      & 100       \\ \hline\hline
      合計         & 1,100    & 1,100     \\ \hline
  \end{tabular}
\end{table}

\section{未登録クラスの検出に対する提案手法の評価}
\label{sec:detect}

\subsection{実験条件}

この研究では，IFORフレームワークにおける特徴抽出器の有効性を評価するために，異なる性質を持つCNNベースおよびViTベースの特徴抽出器について検証を行う．
CNNベースのモデルには，18層の深さと約1,170万のパラメータを有するResNet18 \cite{resnet}を採用した．
ResNet18は，小規模から中規模のデータセットに対して，十分な分類性能を維持しながら効率的な処理が可能なアーキテクチャとして知られている．
一方，ViTベースのモデルとして，Data-efficient image Transformers (DeiT) \cite{deit} の最軽量モデルであるdeit\_tiny\_patch16\_224を利用した．
このアーキテクチャは，約500万のパラメータを有し，モデルのスケールと計算効率の両立を実現している．
DeiTは，入力画像を16$\times$16 pix.のパッチに分割し，自己注意機構を適用することによって特徴抽出を行う．
これらのモデルはImageNetまたはFDSLのデータセットを用いた事前学習が適用されており，
それぞれの転移学習手法がIFORフレームワークにおけるモデル性能に及ぼす影響について検証することが可能である．

IFORでは，特定の地域に生息する野生動物の画像を大量に収集することが困難な状況を想定している．
そのため，本研究で用いるデータ設定は，極めて厳しい条件である5-Way, 1-Shot問題として定義している．
本実験設定は，新規地域において，わずか1クラスあたり1枚の画像を収集するだけで，モデルが利用できる状況を仮定しており，システムの初期導入時に想定される最小限のデータ条件である．

学習の際には，各エピソードにつき学習用データセットから10クラスがランダムに選択される．
このうち5クラスは登録クラス，残りの5クラスは未登録クラスとして設定される．
サポートセットとクローズドクエリセットには登録クラスからのデータのみが使用され，オープンクエリセットには未登録クラスからのデータが割り当てられる．

メタ学習の各学習エピソードでは，学習用データセットからサポートセットとして1クラスにつき1枚，クローズドクエリセットとして1クラスにつき15枚，
オープンクエリセットとして1クラスにつき15枚の画像を用いる．
これに加えて，75枚の画像がベースデータとしてランダムに選択される．
最適化関数にはAdamを採用し，入力画像は224$\times$224 pix. にリサイズを行った．
学習エピソードの総数は30,000エピソードとし，学習過程でモデルが局所最適解に陥るのを防ぐため，10,000エピソード時と20,000エピソード時に学習率を0.1倍ずつ減少させた．
本実験では，様々な転移学習手法や特徴抽出器の組み合わせを検証するため，各実験設定における最適な初期学習率が異なる．
そこで，初期学習率を$10^{-6}$から$10^{-2}$まで10倍ずつ増加させて最適な学習率の探索を行い，最も高い精度が得られた重みを採用した．

評価実験では，新規地域で収集された限られたデータによる実運用を想定し，評価用データセットを用いて実用的な条件下でのモデルの性能評価を行った．
本実験は5-Way，1-Shot問題として定義され，5つのクラスにそれぞれ1枚ずつ，合計5枚のサポートセットによる評価を行った．
クエリセットも学習時と同様に，サポートセットと同一の5クラスを登録クラス，サポートセットと異なる5クラスを未登録クラスとして設定した．
具体的な評価手順は次の通りである．

まず，評価用データセットからランダムに選択された5つのクラスから，それぞれ1枚の画像をプロトタイプとしてモデルに登録する．
次に，特徴抽出器によって得られたクローズドクエリセットの特徴ベクトルを距離が最も近いプロトタイプのクラスへと分類し，正しく分類できた割合に基づいて分類精度を測定する．
オープンクエリデータを未登録クラスとして検出するモデルの能力は，AUROC (Area Under the Receiver Operating Characteristic Curve)指標により評価される．
AUROCは，クエリデータとプロトタイプ間の距離スコアに対する閾値を変化させた際の，偽陽性率 (False Positive Rate, FPR)と
真陽性率 (True Positive Rate, TPR)の関係をプロットしたグラフの曲線下の面積として定義される．
この指標は，0から1の範囲をとり，1に近いほど未登録クラスに対する検出精度が高いことを示す．
ここで，TPRはオープンクエリデータを正しく未登録クラスとして検出できたサンプルの割合を表し，FPRは登録クラスを誤って未登録クラスとして検出したサンプルの割合を示す．
評価用データセットからのクラス選択が特定のクラスに集中することで生じうるバイアスなどを排除し，実験結果の妥当性を担保するため，この評価手順を10,000エピソードにわたり実施した．

\subsection{実験結果及び考察}

本実験では，異なる特徴抽出器であるResNet18とViTについて，ImageNet転移学習，フラクタル転移学習，転移学習なしの3つの実験条件で比較を行った．
なお，転移学習なしの条件では，モデルの重みをランダムに初期化し，WCSデータセットを用いたメタ学習によりモデルのパラメータをスクラッチから更新した．
これらの条件下におけるIFORに対するそれぞれの実験結果を表 \ref{tbl:exp1}に示す．
% 
\begin{table}[tbp]
  \centering
  \caption{IFORに対する各特徴抽出器と転移学習の組み合わせによる実験結果}
  \label{tbl:exp1}

  \begin{tabular}{cc||c|c|c|c|c|c}
      \hline
      \multicolumn{2}{c||}{学習方法}       & \multicolumn{6}{c}{メタ学習 (PEELER)}                        \\ \hline
      \multicolumn{2}{c||}{特徴抽出器}     & \multicolumn{3}{c|}{ResNet18} & \multicolumn{3}{c}{ViT}     \\ \hline
      \multicolumn{2}{c||}{転移学習}       &  ImageNet  &  FDSL  &  なし   &   ImageNet    & FDSL & なし  \\ \hline\hline
      \multirow{2}{*}{赤外線画像} & 分類精度 &    45.8    &  38.8  &  38.6  & \textbf{51.0} & 36.5 & 36.2 \\
                                & AUROC   &    58.4    &  54.3  &  56.3  & \textbf{61.0} & 54.4 & 54.5 \\ \hline
      \multirow{2}{*}{可視光画像} & 分類精度 &    53.0    &  32.8  &  33.2  & \textbf{60.2} & 32.7 & 31.4 \\
                                & AUROC   &    60.8    &  55.2  &  54.3  & \textbf{67.8} & 54.6 & 53.9 \\ \hline
  \end{tabular}
\end{table}
% 
% 気になる
表 \ref{tbl:exp1}に示す全ての実験において，モデルの学習にメタ学習を適用した．
各特徴抽出器に対する実験結果から，赤外線画像の分類においてViTがResNet18よりも高い精度を示すことが明らかとなった．
% 両モデルともImaeNetによる転移学習を用いた場合に最も高い精度が得られた．
ただし，転移学習なしの条件下では，ViTはResNet18よりも低い精度を示した．
これは，転移学習を適用しない場合，ViTよりもCNNベースのモデルの方が優れた性能を示すという従来の知見 \cite{vit}と一致する結果である．

% 表 \ref{tbl:exp1}から，ResNet18とViTの両方で，ImageNetを用いた転移学習により，赤外画像と可視光画像のデータセットで最も高い精度が達成されたことがわかる．
表 \ref{tbl:exp1}より，ResNet18とViTの両モデルにおいて，ImageNetを用いた転移学習が赤外線画像と可視光画像の双方のデータセットに対して最も高い精度を示した．
FDSLによる転移学習は，転移学習なしの場合と比較して，赤外線画像と可視光画像ともに精度をわずかに改善しただけであった．
このような傾向が見られた背景として，フラクタル画像のドメインが自然画像とは異なることから，赤外線画像や可視光画像の動物分類タスクにおいて効果が限定的となった可能性が考えられる．
一方で，FDSLを用いた先行研究では，自然画像で構成されるCIFAR10データセットにおいて，FDSLによって転移学習されたモデルがImageNetを用いて転移学習したモデルを凌駕する性能を示している \cite{fdsl}．
この知見から，FDSLを赤外線画像分類に効果的に適用するためには，自然画像で構成されたデータセットによるファインチューニングが必要である可能性を示唆している．

本研究では，IFORフレームワークにおけるメタ学習の影響も検証しており，その結果を表 \ref{tbl:exp2}に示す．
% 
\begin{table}[tbp]
  \centering
  \caption{ImageNet転移学習を用いた場合のViTによる各学習方法のIFORに対する実験結果}
  \label{tbl:exp2}
  \begin{tabular}{cc||c|c|c}
      \hline
      \multicolumn{2}{c||}{特徴抽出器}       &          \multicolumn{3}{c}{ViT}                \\ \hline
      \multicolumn{2}{c||}{転移学習}         &          \multicolumn{3}{c}{ImageNet}           \\ \hline
      \multicolumn{2}{c||}{学習方法}         & メタ学習 (PEELER)  & 従来法 (ミニバッチ学習) & なし  \\ \hline\hline
      \multirow{2}{*}{赤外線画像} & 分類精度   &  \textbf{51.0}   &        39.9          & 39.6  \\
                                & AUROC     &  \textbf{61.0}   &        53.3          & 55.0  \\ \hline
  \end{tabular}
\end{table}
% 
本実験では，ImageNetで事前学習されたモデルを特徴抽出器として使用し，WCSデータセットを用いて動物分類タスクのためにファインチューニングを行った．
なお，「学習方法」によってファインチューニングのアプローチが異なることに注意が必要である．
メタ学習の場合，分類ヘッドを使用せずに各エピソードで特徴抽出器をファインチューニングした．
具体的には，特徴空間におけるサポートセットとクローズドクエリセット間，および，サポートセットとオープンクエリセット間の距離に基づいて，特徴抽出器のパラメータを更新した．
一方，従来の学習手法であるミニバッチ学習の場合，28個のクラスノードを持つ分類ヘッドを用いて，特徴抽出器を含むすべての層を更新した．
この手法は，ImageNetを用いた事前学習と，ターゲットタスクデータセットを用いたミニバッチ学習を組み合わせた一般的な学習アプローチであり，本研究におけるベースラインとして位置付けされる．
また，学習なしの場合，ImageNetで事前学習された重みをWCSデータセットでの追加学習を行わずに使用した．
本研究の評価方法は特徴空間上の距離に基づいて分類を行うため，一般的な転移学習で行われる重みの凍結や分類ヘッドの再学習は不要であった．
メタ学習との公平な評価条件を確保するため，学習に使用する画像枚数を統一し，学習エポック数を690エポックに設定した．
初期学習率は$10^{-6}$から$10^{-2}$まで10倍ずつ増加させて最適値の探索を行い，学習過程において，学習率は230エポックごとに0.1倍ずつ減少させた．
IFORフレームワークにおけるモデルの性能評価は，評価用データセットを用いて10エポックごとに検証を行い，得られた最も高い精度を最終的な実験結果として採用した．

表 \ref{tbl:exp2}は，IFORフレームワークにおいて，メタ学習がミニバッチ学習や学習なしの条件と比較して，精度とAUROCの両方を顕著に改善したことを示している．
ミニバッチ学習は，学習なしの条件と比較して分類精度をわずかに向上させたものの，AUROCは低下する結果となった．
% この結果は，未登録クラスの検出を考慮せずに28クラスの赤外線画像を分類する手法がAUROCの低下につながったことを示しており，IFORにおけるメタ学習の有効性を裏付けている．
この結果は，従来の学習手法であるミニバッチ学習が28クラスの識別に特化した特徴抽出器の学習を行うため，特徴空間上の各クラスの分布が未登録クラスの検出に適さない形で最適化されたことを示唆している．
一方，メタ学習では特徴空間における距離関係を直接的に学習することにより，登録クラスの分類と未登録クラスの検出を同時に考慮した特徴表現の獲得が可能となり，結果としてAUROCの向上につながったと考えられる．

% 結論として，本研究は赤外線画像を用いたタスク分類の複雑さを探究し，色情報の欠如による困難さの増大を強調している．
% 可視光画像と赤外線画像に対する分類性能の比較分析により，赤外線画像に関連する課題の高さが確認された．
本研究を通じて，赤外線画像による動物分類タスクの複雑性が明らかとなり，特に色情報の欠如に起因する課題の重要性が示された．
さらに，可視光画像と赤外線画像の分類性能の比較分析により，赤外線画像特有の課題の顕著さが確認された．

また，本研究ではIFORにおける基盤モデルを用いた意味的な特徴抽出の有効性についても検証を行った．
本実験で用いる特徴抽出器のモデル構造には，ViTの標準的なモデルサイズであるViT-Baseを採用した．
基盤モデルについては，ViTベースのCLIPモデルとしてViT-B/16を利用し，以降，これをCLIPと表記する．
一方，比較対象であるテキストを用いた学習が行われていないViTはViT-Baseと表記する．
これらのアーキテクチャは，約8,600万のパラメータを有しており，入力画像を16$\times$16 pix.のパッチに分割し特徴抽出が行われる．
CLIPはインターネットを介して収集された約4億組の画像とテキストのペアから構成されるWebImageText (WIT)という大規模データセットを使用して事前学習されている．
一方で，ViT-BaseはImageNet-1kの拡張番データセットであり，21,000クラスを含むImageNet-21kを用いて事前学習が行われている．

図 \ref{tbl:exp3}にCLIPの追加学習の有無が赤外線画像の分類類性能に与える影響を示している．
% 精度未完成
\begin{table}[tbp]
  \centering
  \caption{IFORにおける基盤モデルの追加学習の有無による分類性能の実験結果}
  \label{tbl:exp3}

  \begin{tabular}{cc||c|c|c|c}
      \hline
      \multicolumn{2}{c||}{特徴抽出器}      & \multicolumn{2}{c|}{CLIP} & \multicolumn{2}{c}{ViT-Base} \\ \hline
      \multicolumn{2}{c||}{転移学習}       &  \multicolumn{2}{c|}{WIT}  & \multicolumn{2}{c}{ImageNet-21k} \\ \hline
      \multicolumn{2}{c||}{学習方法}       & メタ学習 (PEELER) &   なし   & メタ学習 (PEELER) &    なし    \\ \hline\hline
      \multirow{2}{*}{赤外線画像} & 分類精度 &       45.6      &   25.1   &  \textbf{49.2}  &    28.6    \\
                                & AUROC   &       49.6      &   49.9   &  \textbf{50.1}  &    50.2    \\ \hline
  \end{tabular}
\end{table}
% 
実験結果から，IFORにおいてCLIPの性能がViT-Baseに劣ることが明らかとなった．
本実験では，特徴空間上での距離に基づいた評価を行ったが，先行研究 \cite{clip}ではテキストエンコーダを用いた分類を行っている．
CLIPの学習ではテキストの特徴ベクトルと画像の特徴ベクトルとの類似度に基づき分類を行っているため，画像の特徴ベクトルのみを用いた分類には不向きである可能性がある．
この事実から，CLIPをIFORフレームワークに効果的に適用するためには，テキストエンコーダを用いた類似度計算が必要である可能性を示唆している．

% 表 \ref{tbl:exp1}および表 \ref{tbl:exp2}に示された結果は，新規地域においてサポートセットとしてわずか1枚の画像のみを使用するという，極めて厳しい条件下で評価されている．
% しかし，システムの継続的な運用に伴い，サポートとして利用できる画像の数は自然に増加することが期待される．
% このような実運用シナリオを考慮し，図 \ref{fig:k-shot}に示すように，評価時のショット数$K$が分類精度とAUROCに与える影響ついて検証を行った．
% % 
% \begin{figure}[tbp]
%   \centering
%   \begin{subfigure}[b]{0.45\linewidth}
%     \centering
%     \includegraphics[height=0.9\linewidth, keepaspectratio]{image/sec1k-shotVSaccuracy.png}
%     % \caption{coyoteの画像例}
%     \label{fig:sec1Accuracy}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.45\linewidth}
%     \centering
%     \includegraphics[height=0.9\linewidth, keepaspectratio]{image/sec1k-shotVSAUROC.png}
%     % \caption{bobcatの画像例}
%     \label{fig:sec1AUROC}
%   \end{subfigure}
%   \caption{評価時におけるサポートデータの数$K$を増やした時のAccuracyとAUROCの変化}
%   \label{fig:k-shot}
% \end{figure}
% % 
% 本評価では，特徴抽出器としてViT，転移学習手法としてImageNet，学習方法としてメタ学習を採用したモデルを使用した．
% 実験の結果，1クラスあたりの登録データの数を10枚，すなわち，10-shotとすることで，分類精度が75\%を超えることが確認された．
% また，表 \ref{tbl:shot}に各カテゴリーにおける分類精度を示す．

% \begin{table}[tbp]
%   \centering
%   \caption{Shot数を変えた時の各カテゴリの分類精度}
%   \label{tbl:shot}
%   \begin{tabular}{c|c|c|c}
%       \hline
%       動物種    & 1-shot & 5-shot & 10-shot \\ \hline\hline
%       bird     & 39.6   & 63.5   & 67.7    \\
%       bobcat   & 49.7   & 67.7   & 73.0    \\
%       cat      & 35.4   & 53.8   & 60.2    \\
%       coyote   & 41.6   & 57.3   & 63.7    \\
%       deer     & 65.9   & 86.1   & 89.9    \\
%       dog      & 40.5   & 66.4   & 75.2    \\
%       fox      & 45.0   & 62.2   & 67.0    \\
%       opossum  & 62.3   & 77.2   & 80.8    \\
%       rabbit   & 51.5   & 72.9   & 75.6    \\
%       skunk    & 71.1   & 87.3   & 89.6    \\
%       squirrel & 60.9   & 82.9   & 84.8    \\ \hline
%   \end{tabular}
% \end{table}

% IFORにおけるメタ学習の詳細な分析を通じて，本研究における2つの主要な課題が明らかになった．
% 1つ目は動物の姿勢による誤分類である．
% 特に，動物の背面のみが撮影された画像や複雑な姿勢をとる個体の画像など，出現頻度の低い姿勢パターンに対して分類精度の低下が観察された．
% 図 \ref{fig:animalback}は，このような姿勢の影響により誤分類された画像の具体的事例を示している．

% \begin{figure}[tbp]
%   \centering
%   \begin{subfigure}[b]{0.45\linewidth}
%     \centering
%     \includegraphics[height=0.9\linewidth, keepaspectratio]{image/coyote.png}
%     \caption{coyoteの画像例}
%     \label{fig:coyote}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.45\linewidth}
%     \centering
%     \includegraphics[height=0.9\linewidth, keepaspectratio]{image/bobcat.png}
%     \caption{bobcatの画像例}
%     \label{fig:bobcat}
%   \end{subfigure}
%   \caption{姿勢が原因で誤分類された画像例 (正解カテゴリ: coyote, 予測カテゴリ: bobcat)}
%   \label{fig:animalback}
% \end{figure}

% 2つ目の課題は未登録クラスの扱いに関するものである．
% 現在のモデルは，すべての未登録クラスを一括して検出することのみを想定しているが，これらの未登録クラスを将来的に追加学習に活用する場合，各クラスに対するアノテーションが必要となる．
% しかし，現在のシステムでは人手によるアノテーションに多大なコストを要するため，未登録クラス内での多クラス分類が不可欠である．

% これらの課題に対する実用的な解決策として，以下の改善を提案する．
% 動物の姿勢による誤分類に対しては，カメラトラップで連続的に撮影された画像をシーケンスデータとして扱うことが有効である．
% 複数の連続画像の時間的文脈を考慮することで，単一画像からの判断が困難な姿勢に対しても正確な分類が可能となる．
% 未登録クラスの問題に対しては，メタ学習にクラスタリングベースの損失を導入してクラス分布をコンパクトに表現することが，未登録クラス内での多クラス分類の実現に適していると考えられる．
% このアプローチにより，モデルはクラス間の特徴の違いを学習し，以前に見たことのないクラスを正確に分類することが可能になると期待される．

% これらの提案された改善は，モデルの頑健性と実世界での適用可能性を向上させる．
% 現在の制限に対処しながら，IFORフレームワークの強みを維持している．
% これらの提案は，進行中の研究で取り組むべき将来の課題として提示されている．

\section{未登録クラスの多クラス分類に対する損失関数の評価}

\subsection{実験条件}

未登録の動物種に対する分類精度の評価に際し，本実験では新たな評価手法を導入した．
\ref{sec:detect} 節の実験の評価の際，登録クラスから得られるプロトタイプのみを用いて分類精度を求めていたが，
本実験では登録クラスのプロトタイプに加えて，未登録クラスから求められるプロトタイプを用いて未登録クラスに対する分類精度も測る．
ここで，登録クラスから構成されたサポートセットをクローズドサポートセット (closed-support set)と呼び，それから得られるプロトタイプをクローズドプロトタイプ(closed-prototype)と呼ぶ．
同様に，未登録クラスから構成されたサポートセットをオープンサポートセット (open-support set)と呼び，それから得られるプロトタイプをオープンプロトタイプ (open-prototype)と呼ぶ．
図 \ref{fig:openprototype}にクローズドプロトタイプとオープンプロトタイプを用いた評価方法の概要を示している．
% 
\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.8\linewidth, keepaspectratio]{image/open-prototype.png}
  \caption{クローズドプロトタイプとオープンプロトタイプを用いた評価方法}
  \label{fig:openprototype}
\end{figure}
% 
本評価方法では，特徴抽出器に入力された画像は特徴空間上にプロットされ，最も近いプロトタイプのクラスに分類される．
具体的には，特徴点とプロトタイプ間の距離に基づき，オープンプロトタイプに近ければそのプロトタイプが属する未登録クラスに分類され，クローズドプロトタイプに近ければそのプロトタイプが属する登録クラスに分類される．
評価設定として5-Way，1-Shot問題を採用し，評価データから各エピソードにおいて10クラスをランダムに選択し，そのうち5クラスを登録クラス，残りの5クラスを未登録クラスとして使用した．
また，クローズドサポートセットとオープンサポートセットには1クラスにつき1枚の画像を用いて評価を行った．

\subsection{実験結果及び考察}

表 \ref{tbl:exp4}に未登録の動物種の分類結果を示す．
% 
% 精度怪しい
\begin{table*}[tbp]
  \centering
  \caption{IFORにおけるk-means損失とBC損失のアブレーション結果}
  \label{tbl:exp4}
  \footnotesize
  \setlength{\tabcolsep}{4pt} % 列幅のパディングをdefaltの6ptから4ptに変更
  \begin{tabular}{c||c|c|c|c|c|c} \hline
    学習フレームワーク                                        &    k-means   &      BC      & Closed Accuracy (\%) & Open Accuracy (\%) & All Accuracy (\%) &      AUROC     \\ \hline\hline
    \multirow{4}{*}{PEELER}                                &              &              &         38.3         &        37.8        &        38.1       & \textbf{49.7} \\ \cline{2-7}
                                                           & $\checkmark$ &              &         38.3         &        37.8        &        38.1       & \textbf{49.7} \\ \cline{2-7}
                                                           &              & $\checkmark$ &         38.3         &        37.8        &        38.1       & \textbf{49.7} \\ \cline{2-7}
                                                           & $\checkmark$ & $\checkmark$ &         39.0         &        38.3        &        38.7       &      49.6     \\ \hline
    \multirow{3}{*}{\shortstack[c]{PEELER\\(w/o 分類損失)}} & $\checkmark$ &              &         39.3         &    \textbf{39.2}   &        39.2       &      49.6     \\ \cline{2-7}
                                                           &              & $\checkmark$ &     \textbf{39.5}    &    \textbf{39.2}   &    \textbf{39.3}  &      49.6     \\ \cline{2-7}
                                                           & $\checkmark$ & $\checkmark$ &         39.3         &    \textbf{39.2}   &        39.2       &      49.6     \\ \hline
  \end{tabular}
\end{table*}
% 
本実験では，距離学習に基づくFSL損失とOSR損失に加え，分類損失を組み合わせた既存のPEELERフレームワークをベースラインとし，k-means損失，BC損失の有効性について検証を行った．
すべての実験において，ImageNetで事前学習済みのViTを特徴抽出器として用いた．
既存手法のPEELERにk-means損失，または，BC損失を個別に組み合わせた場合，モデルの分類精度はベースラインとほぼ同等の性能であった．
一方，k-means損失とBC損失の両方をベースラインのPEELERに組み合わせることによって，ベースラインより高い性能を達成することが確認された．

次に，分類損失を用いないPEELERにk-means損失およびBC損失を組み合わせた結果を確認すると，k-means損失のみ，BC損失のみ，
k-means損失およびBC損失を組み合わせた場合のいずれにおいてもベースラインの精度より高い精度となることが確認された．
この結果は，IFORの5-Way，1-Shot問題において，分類損失を用いずにk-means損失やBC損失を単体で用いることの有効性を示している．

% 表 \ref{tbl:exp4}は，新規地域において1枚の画像のみを用いた学習という極めて厳しい条件下での評価結果である．
% ここから，前節の結果を載せるようにする
% しかし，実運用においては，新規地域で撮影された画像が徐々に蓄積されていくため，学習に利用可能な画像枚数 (サポートセット) は増加していく．
% Fig. 5では，評価時の学習画像数𝐾を変化させた場合の分類精度の推移を示した．

\ref{sec:detect}節の表 \ref{tbl:exp1}および表 \ref{tbl:exp2}，本節の表 \ref{tbl:exp4}に示された結果は，新規地域においてサポートセットとしてわずか1枚の画像のみを使用するという，極めて厳しい条件下で評価されている．
しかし，システムの継続的な運用に伴い，サポートセットとして利用できる画像の数は自然に増加することが期待される．
このような実運用シナリオを考慮し，図 \ref{fig:k-shot}に示すように，評価時のショット数$K$が分類精度とAUROCに与える影響ついて検証を行った．
% 
\begin{figure}[tbp]
  \centering
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[height=0.9\linewidth, keepaspectratio]{image/sec2k-shotVSaccuracy.png}
    \caption{All Accuracyの推移}
    \label{fig:sec2Accuracy}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[height=0.9\linewidth, keepaspectratio]{image/sec2k-shotVSAUROC.png}
    \caption{AUROCの推移}
    \label{fig:sec2AUROC}
  \end{subfigure}
  \caption{評価時におけるサポートデータの数$K$を増やした時のAll AccuracyとAUROCの変化}
  \label{fig:k-shot}
\end{figure}
% 
% 本評価では，特徴抽出器としてViT，転移学習手法としてImageNet，学習方法としてメタ学習を採用したモデルを使用した．
% 実験の結果，1クラスあたりの登録データの数を10枚，すなわち，10-shotとすることで，分類精度が75\%を超えることが確認された．
この評価では，特徴抽出器としてViT，転移学習手法としてImageNet，学習方法としてメタ学習，損失関数として，FSL損失，OSR損失，BC損失を採用したモデルを使用した．
実験の結果，表 \ref{fig:sec2Accuracy}より，1クラスあたりのサポートデータを20枚，すなわち，20-Shotとすることで，分類精度が約70\%を超えることが確認された．
また，表 \ref{tbl:shot}に各カテゴリーにおける分類精度を示す．

% 表を修正する
\begin{table}[tbp]
  \centering
  \caption{Shot数を変えた時の各カテゴリの分類精度}
  \label{tbl:shot}
  \begin{tabular}{c|c|c|c}
      \hline
      動物種    & 1-shot & 5-shot & 10-shot \\ \hline\hline
      bird     & 39.6   & 63.5   & 67.7    \\
      bobcat   & 49.7   & 67.7   & 73.0    \\
      cat      & 35.4   & 53.8   & 60.2    \\
      coyote   & 41.6   & 57.3   & 63.7    \\
      deer     & 65.9   & 86.1   & 89.9    \\
      dog      & 40.5   & 66.4   & 75.2    \\
      fox      & 45.0   & 62.2   & 67.0    \\
      opossum  & 62.3   & 77.2   & 80.8    \\
      rabbit   & 51.5   & 72.9   & 75.6    \\
      skunk    & 71.1   & 87.3   & 89.6    \\
      squirrel & 60.9   & 82.9   & 84.8    \\ \hline
  \end{tabular}
\end{table}

各実験結果の詳細な分析により，本研究における主要な課題として動物の姿勢による誤分類が明らかとなった．
特に，動物の背面のみが撮影された画像や複雑な姿勢をとる個体の画像など，出現頻度の低い姿勢パターンに対して分類精度の低下が観察された．
図 \ref{fig:animalback}は，このような姿勢の影響により誤分類された画像の具体的事例を示している．
この課題に対する実用的な解決策としては，カメラトラップにより連続的に撮影された画像群をシーケンスデータとして扱うアプローチが有効である．
複数の連続画像における時間的文脈を考慮することで，単一の画像からでは判断が困難な姿勢に対しても高精度な分類が期待できる．
この改善案は，モデルの頑健性および実環境における適用可能性の向上に寄与しつつ，IFORフレームワークの本質的な利点を維持することが可能である．、

\begin{figure}[tbp]
  \centering
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[height=0.9\linewidth, keepaspectratio]{image/coyote.png}
    \caption{coyoteの画像例}
    \label{fig:coyote}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[height=0.9\linewidth, keepaspectratio]{image/bobcat.png}
    \caption{bobcatの画像例}
    \label{fig:bobcat}
  \end{subfigure}
  \caption{姿勢が原因で誤分類された画像例 (正解カテゴリ: coyote, 予測カテゴリ: bobcat)}
  \label{fig:animalback}
\end{figure}

% ここから参考文献bibtexの設定
\bibliographystyle{../kishiIEEEtr}
\bibliography{../references}

\end{document}